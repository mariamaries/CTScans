{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "*CTScans.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nZgw8Sv993E"
      },
      "source": [
        "Importing the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqD8dFDqaNLv"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.image as img\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx206FTKbPrJ",
        "outputId": "b3bc25b7-f71c-41d1-9af3-d96f30bf90a2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaACHqdv-Dru"
      },
      "source": [
        "Importing the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjsRi1oBumtQ"
      },
      "source": [
        "path_train = '/content/gdrive/MyDrive/ai-unibuc-23-31-2021/train'\n",
        "path_val = '/content/gdrive/MyDrive/ai-unibuc-23-31-2021/validation'\n",
        "path_test = '/content/gdrive/MyDrive/ai-unibuc-23-31-2021/test'\n",
        "train_data = pd.read_csv('/content/gdrive/MyDrive/ai-unibuc-23-31-2021/train.txt', sep =\",\", names=['path', 'label'])\n",
        "val_data = pd.read_csv('/content/gdrive/MyDrive/ai-unibuc-23-31-2021/validation.txt', sep =\",\", names=['path', 'label'])\n",
        "test_data = pd.read_csv('/content/gdrive/MyDrive/ai-unibuc-23-31-2021/test.txt', sep =\",\", names=['path', 'label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5c3c4Pb0wdY"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsbLK4-R-VyS"
      },
      "source": [
        "Transforming and normalizing the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-dEeAVZr1Y8"
      },
      "source": [
        "transformer = transforms.Compose([\n",
        "                        transforms.ToPILImage(),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.5,), (0.5,))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RkjZwDDvMRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbefb18b-6b06-4daa-b528-a0b091ab16be"
      },
      "source": [
        "train_set = []\n",
        "val_set = []\n",
        "test_set = []\n",
        "for i in range(len(train_data)):\n",
        "  img_path, label = train_data['path'][i], train_data['label'][i]\n",
        "  img_path = os.path.join(path_train, train_data['path'][i])\n",
        "  image = img.imread(img_path)\n",
        "  image = transformer(image)\n",
        "  train_set.append([image, label])\n",
        "\n",
        "for i in range(len(val_data)):\n",
        "  img_path, label = val_data['path'][i], val_data['label'][i]\n",
        "  img_path = os.path.join(path_val, val_data['path'][i])\n",
        "  image = img.imread(img_path)\n",
        "  image = transformer(image)\n",
        "  val_set.append([image, label])\n",
        "\n",
        "# for i in range(len(test_data)):\n",
        "#   img_path = test_data['path'][i]\n",
        "#   img_path = os.path.join(path_test, test_data['path'][i])\n",
        "#   image = img.imread(img_path)\n",
        "#   image = transformer(image)\n",
        "#   test_set.append(image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:132: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt0j4qIi-f0m"
      },
      "source": [
        "Creating the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54cpgwV-1Hwq"
      },
      "source": [
        "batch_size = 100\n",
        "batch_size1 = len(test_data)\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,drop_last=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True,drop_last=True)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size1)\n",
        "first_train_batch_imgs, first_train_batch_labels = next(iter(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtyG4ig0-oGB"
      },
      "source": [
        "Defining the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nSk8wt6rH7A"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.conv1 = nn.Conv2d(1, 15, 3, 1, padding=1)\n",
        "      self.conv2 = nn.Conv2d(15, 30, 3, 1, padding=1)\n",
        "      self.conv3 = nn.Conv2d(30, 50, 3, 1, padding=1)\n",
        "      self.fc1 = nn.Linear(6*6*50, 100)\n",
        "      self.dropout1 = nn.Dropout(0.5)\n",
        "      self.fc2 = nn.Linear(100, 3)\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = F.max_pool2d(x, 2, 2)\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = F.max_pool2d(x, 2, 2)\n",
        "      x = F.relu(self.conv3(x))\n",
        "      x = F.max_pool2d(x, 2, 2)\n",
        "      x = x.view(-1, 6*6*50)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = self.dropout1(x)\n",
        "      x = self.fc2(x)\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMasF-Hz-q_f"
      },
      "source": [
        "Defining two functions: one for training, one for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7YM6dRLz_KC"
      },
      "source": [
        "def train(model, device, train_loader):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for index, (image, label) in enumerate(train_loader):\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "  \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.detach().cpu().numpy())\n",
        "        if index % 150 == 0:\n",
        "            print('epoch:' + str(epoch))\n",
        "            print('train loss:' + str(loss.item()))\n",
        "    return sum(train_losses)/len(train_losses)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    corrects = 0\n",
        "    prediction = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        n = 0\n",
        "        for image, label in test_loader:\n",
        "            image, label = image.to(device), label.to(device)\n",
        "            output = model(image)\n",
        "            loss = criterion(output, label) \n",
        "            test_loss += loss\n",
        "\n",
        "            _, pred = torch.max(output, 1)\n",
        "            corrects += torch.sum(pred == label.data)\n",
        "            pred = pred.detach().cpu().numpy()\n",
        "            prediction.append(pred)\n",
        "            label = label.detach().cpu().numpy()\n",
        "            labels.append(label)\n",
        "            n += 1\n",
        "\n",
        "    test_loss /= n\n",
        "    test_accuracy = corrects / n\n",
        "\n",
        "    print('test loss:' + str(test_loss.item()))\n",
        "    print('accuracy:' + str(test_accuracy.item()) +'\\n')\n",
        "    return labels, prediction, test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LplCXoek-xl5"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOq43rxA0DD4"
      },
      "source": [
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay = 0.01, amsgrad = True)\n",
        "\n",
        "losses_train = []\n",
        "losses_val = []\n",
        "accuracy_val = []\n",
        "for epoch in range(25):\n",
        "    train_loss = train(model, device, train_loader)\n",
        "    labels, prediction, val_loss, val_accuracy = test(model, device, val_loader)\n",
        "    losses_train.append(train_loss)\n",
        "    losses_val.append(val_loss)\n",
        "    accuracy_val.append(val_accuracy)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(losses_train,label='train_loss',color='red')\n",
        "plt.plot(losses_val,label='val_loss',color='blue')\n",
        "plt.figure(2)\n",
        "plt.plot(accuracy_val,label='val_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpzUAIgIy5UV"
      },
      "source": [
        "labels = np.array(labels)\n",
        "labels = labels.flatten()\n",
        "prediction = np.array(prediction)\n",
        "prediction = prediction.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ngvK5wkwSTw",
        "outputId": "f69dd512-bc4f-4d6e-87f1-c0e9f6cc0c98"
      },
      "source": [
        "print(confusion_matrix(labels,prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1242  171   87]\n",
            " [ 231  920  349]\n",
            " [ 144  326 1030]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTGoFncQ-3zr"
      },
      "source": [
        "Creating a predictions document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da2votdaQpIW"
      },
      "source": [
        "predictions = []\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    data = data.to(device)\n",
        "    print(data)\n",
        "    output = model(data) \n",
        "    pred = output.argmax(1)\n",
        "    predictions.append(pred.cpu().detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dol2THLEhbIH"
      },
      "source": [
        "import numpy as np\n",
        "predictions = np.array(predictions)\n",
        "pred2 = []\n",
        "for i in predictions:\n",
        "    pred2.append(i)\n",
        "predictions = predictions.flatten()\n",
        "\n",
        "fout = open('/content/gdrive/MyDrive/ai-unibuc-23-31-2021/submission.txt', 'w')\n",
        "fout.write('id,label\\n')\n",
        "for index, row in test_data.iterrows():\n",
        "    fout.write(row['path'] + ',' + str(pred2[index]) + '\\n')\n",
        "fout.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}